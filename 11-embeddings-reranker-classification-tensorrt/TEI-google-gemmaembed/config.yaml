# this file was autogenerated by `generate_templates.py` - please do change via template only
base_image:
  image: baseten/text-embeddings-inference-mirror:hopper-1.8.1
docker_server:
  liveness_endpoint: /health
  predict_endpoint: /v1/embeddings
  readiness_endpoint: /health
  server_port: 7997
  start_command:
    bash -c "truss-transfer-cli && text-embeddings-router --port 7997
    --model-id /app/model_cache/cached_model --max-client-batch-size 128 --max-concurrent-requests
    1024 --max-batch-tokens 16384 --auto-truncate --tokenization-workers 3"
model_cache:
  - repo_id: google/embeddinggemma-300m
    revision: main
    use_volume: true
    volume_folder: cached_model
model_metadata:
  example_model_input:
    encoding_format: float
    input: text string
    model: model
model_name: TEI-google-gemmaembed
python_version: py39
resources:
  accelerator: H100_40GB
  cpu: "1"
  memory: 2Gi
  use_gpu: true
runtime:
  is_websocket_endpoint: false
  transport:
    kind: http
