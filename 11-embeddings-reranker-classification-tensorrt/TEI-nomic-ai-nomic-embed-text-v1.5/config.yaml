base_image:
  image: baseten/text-embeddings-inference-mirror:86-1.6
docker_server:
  liveness_endpoint: /health
  predict_endpoint: /v1/embeddings
  readiness_endpoint: /health
  server_port: 7997
  start_command: bash -c "truss-transfer-cli && text-embeddings-router --port 7997
    --model-id /app/model_cache/cached_model --max-client-batch-size 128 --max-concurrent-requests
    40 --max-batch-tokens 16384"
model_cache:
- repo_id: nomic-ai/nomic-embed-text-v1.5
  revision: main
  use_volume: true
  volume_folder: cached_model
model_metadata:
  example_model_input:
    encoding_format: float
    input: text string
    model: model
model_name: TEI-nomic-ai-nomic-embed-text-v1.5-truss-example
python_version: py39
resources:
  accelerator: A10G
  cpu: '1'
  memory: 2Gi
  use_gpu: true
runtime:
  predict_concurrency: 40
